---
description: 'جب LLMs روبوٹس سے ملتے ہیں: Vision-Language-Action ماڈلز کو سمجھنا جو
  روبوٹس کو سمجھنے، استدلال کرنے، اور عمل کرنے کی cognitive abilities فراہم کرتے ہیں۔'
id: vla-introduction
keywords:
- VLA
- Vision-Language-Action
- LLM
- robotics
- AI
- cognitive robots
module: 4
sidebar_position: 1
tags:
- module-4
- introduction
- advanced
title: 'Module 4: Vision-Language-Action (VLA)'
---

# ماڈیول 4: Vision-Language-Action (VLA)

> **TL;DR:** آپ کا روبوٹ دیکھ اور حرکت کر سکتا ہے، لیکن یہ *سمجھتا* نہیں۔ VLA ماڈلز وژن، لینگویج، اور ایکشن کو یکجا کرتے ہیں تاکہ ایسے روبوٹس بنائے جا سکیں جو کمانڈز کی تشریح کر سکیں، اپنے ماحول کے بارے میں استدلال کر سکیں، اور پیچیدہ رویوں کی منصوبہ بندی کر سکیں۔ یہ وہ جگہ ہے جہاں روبوٹس کو دماغ ملتے ہیں۔

---

## گمشدہ ٹکڑا

آپ نے بہت کچھ بنایا ہے:

| ماڈیول | کامیابی |
|--------|-------------|
| **ماڈیول 1** | ROS 2 کے بنیادی اصول |
| **ماڈیول 2** | سمولیشن ماحول |
| **ماڈیول 3** | GPU پرسیپشن + نیویگیشن |
| **ماڈیول 4** | **???** |

آپ کا روبوٹ نیویگیٹ کر سکتا ہے، لیکن اگر آپ اسے "مجھے کافی لا دو" کہیں تو وہ خالی نظروں سے گھورتا ہے۔ اس میں کمی ہے:

-   **لینگویج انڈرسٹینڈنگ** — "کافی" کا کیا مطلب ہے؟
-   **ویژول گراؤنڈنگ** — کون سی چیز کافی ہے؟
-   **ٹاسک پلاننگ** — کون سے اقدامات درکار ہیں؟
-   **ایکشن جنریشن** — ان اقدامات کو کیسے انجام دیا جائے؟

**VLA ماڈلز یہ سب حل کرتے ہیں۔**

---

## VLA کیا ہے؟

VLA (Vision-Language-Action) ایک ایسا پیراڈائم ہے جو یکجا کرتا ہے:

```
┌─────────────────────────────────────────────────────────────────┐
│                                                                  │
│   ┌──────────┐    ┌──────────┐    ┌──────────┐                │
│   │  VISION  │    │ LANGUAGE │    │  ACTION  │                │
│   │          │    │          │    │          │                │
│   │ What do  │    │ What am  │    │ What     │                │
│   │ I see?   │    │ I asked? │    │ should   │                │
│   │          │    │          │    │ I do?    │                │
│   └────┬─────┘    └────┬─────┘    └────┬─────┘                │
│        │               │               │                       │
│        └───────────────┼───────────────┘                       │
│                        │                                        │
│                ┌───────▼───────┐                               │
│                │   VLA MODEL   │                               │
│                │               │                               │
│                │  Unified AI   │                               │
│                │    Brain      │                               │
│                └───────────────┘                               │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### تین اجزاء

| جزو | ان پٹ | آؤٹ پٹ | کردار |
|-----------|-------|--------|------|
| **Vision** | کیمرے کی تصاویر | منظر کی سمجھ | "میں ایک میز پر ایک سرخ کپ دیکھ رہا ہوں" |
| **Language** | متن/تقریر | ارادے کی سمجھ | "صارف چاہتا ہے کہ میں کپ اٹھاؤں" |
| **Action** | ٹاسک کا سیاق و سباق | روبوٹ کمانڈز | `move_arm(target=red_cup)` |

---

## VLA انقلاب

### VLA سے پہلے (2020)

```
User: "Pick up the red cup"
Robot: *404 task not found*
```

روایتی طریقہ کار کی ضرورت تھی:
-   ہاتھ سے کوڈ کیے گئے آبجیکٹ ڈیٹیکٹرز
-   پہلے سے طے شدہ ٹاسک ٹیمپلیٹس
-   واضح ایکشن سیکوینسز
-   ہر ٹاسک کے لیے مہینوں کی انجینئرنگ

### VLA کے بعد (2024)

```
User: "Pick up the red cup"
Robot: *understands, locates, grasps*
```

VLA ماڈلز:
-   مظاہروں سے سیکھتے ہیں
-   نئی اشیاء پر عمومی بناتے ہیں
-   قدرتی زبان کو ہینڈل کرتے ہیں
-   نئی صورتحال کے بارے میں استدلال کرتے ہیں

---

## اہم VLA آرکیٹیکچرز

### 1. RT-2 (Google DeepMind)

روبوٹ کے ایکشنز کو لینگویج ماڈل میں ٹوکنز کے طور پر ٹریٹ کرتا ہے:

```
Input:  "Pick up the bottle" + [Image]
Output: "1 128 91 241 1 128 91" → robot actions
```

### 2. OpenVLA

Open X-Embodiment ڈیٹاسیٹ پر تربیت یافتہ اوپن سورس VLA:

```python
from openvla import OpenVLA

model = OpenVLA.from_pretrained("openvla-7b")
action = model.predict(image, "pick up the apple")
```

### 3. NVIDIA Isaac (GR00T)

اینڈ ٹو اینڈ ہیومنائڈ فاؤنڈیشن ماڈل:

```
Speech/Text → Understanding → Action Tokens → Robot Motion
```

---

## VLA کیسے کام کرتا ہے

### آرکیٹیکچر کا جائزہ

```
┌─────────────────────────────────────────────────────────────────┐
│                        VLA MODEL                                 │
│                                                                  │
│  ┌──────────────────────────────────────────────────────────┐   │
│  │                   Vision Encoder                          │   │
│  │               (ViT, DINOv2, SigLIP)                       │   │
│  │                                                           │   │
│  │     [Image] → [Visual Tokens: v1, v2, v3, ...]          │   │
│  └─────────────────────────┬────────────────────────────────┘   │
│                            │                                     │
│  ┌──────────────────────────▼───────────────────────────────┐   │
│  │                 Language Model                            │   │
│  │              (LLaMA, Qwen, Gemma)                         │   │
│  │                                                           │   │
│  │  [Visual Tokens] + [Text Tokens] → [Action Tokens]       │   │
│  └─────────────────────────┬────────────────────────────────┘   │
│                            │                                     │
│  ┌──────────────────────────▼───────────────────────────────┐   │
│  │                  Action Decoder                           │   │
│  │           (MLP, Diffusion, Tokenizer)                     │   │
│  │                                                           │   │
│  │    [Action Tokens] → [Joint Positions/Velocities]        │   │
│  └──────────────────────────────────────────────────────────┘   │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### جادو: کراس-موڈل ریزننگ

```python
# Pseudo-code of VLA inference
def vla_forward(image, instruction):
    # Encode vision
    visual_features = vision_encoder(image)  # [N, D]
    
    # Encode language
    text_tokens = tokenizer(instruction)
    text_features = language_encoder(text_tokens)  # [M, D]
    
    # Fuse modalities
    fused = cross_attention(visual_features, text_features)
    
    # Decode to actions
    action_tokens = action_decoder(fused)
    
    # Convert to robot commands
    joint_positions = detokenize_actions(action_tokens)
    
    return joint_positions
```

---

## ہیومنائڈز کے لیے یہ کیوں اہم ہے

ہیومنائڈز خاص طور پر VLA کے لیے موزوں ہیں کیونکہ:

| عنصر | یہ کیوں مدد کرتا ہے |
|--------|--------------|
| **انسانی جیسی شکل** | انسانی مظاہروں سے سیکھ سکتے ہیں |
| **ماہر ہاتھ** | پیچیدہ مینیپولیشن کے کام |
| **بھرپور حسی ان پٹ** | سر پر لگے کیمرے = قدرتی نقطہ نظر |
| **عام مقصد** | ایک ماڈل، کئی کام |

### خواب

```python
# The future is this simple
robot.execute("Please make me a sandwich")
# Robot: Understands, plans, navigates, manipulates, serves
```

---

## ماڈیول 4 روڈ میپ

| سیکشن | موضوع | آپ کیا سیکھیں گے |
|---------|-------|-------------------|
| **4.1** | تعارف | VLA کے بنیادی اصول (آپ یہاں ہیں) |
| **4.2** | وائس ٹو ایکشن | Whisper کے ساتھ اسپیچ ریکگنیشن |
| **4.3** | کاگنیٹو پلاننگ | ٹاسک ڈیکمپوزیشن کے لیے LLMs |
| **4.4** | کیپ اسٹون | خود مختار ہیومنائڈ |
| **4.5** | آگے کیا ہے | فزیکل AI کا مستقبل |

---

## پیشگی ضروریات

شروع کرنے سے پہلے، یقینی بنائیں کہ آپ کے پاس ہے:

-   ✅ ماڈیول 1-3 مکمل کر لیے ہیں
-   ✅ نیورل نیٹ ورکس کی سمجھ (بنیادی باتیں)
-   ✅ Python میں مہارت
-   ✅ 8GB+ VRAM کے ساتھ GPU (انفرنس کے لیے)
-   ✅ OpenAI API key یا مقامی LLM سیٹ اپ

---

## وہ ٹولز جو ہم استعمال کریں گے

| ٹول | مقصد |
|------|---------|
| **OpenAI Whisper** | اسپیچ ٹو ٹیکسٹ |
| **LangChain** | LLM آرکیسٹریشن |
| **GPT-4 / Claude / LLaMA** | کاگنیٹو ریزننگ |
| **OpenVLA** | Vision-Language-Action |
| **ROS 2** | روبوٹ انٹیگریشن |

---

## فلسفہ

یہ ماڈیول روبوٹس کو **ایجنسی** دینے کے بارے میں ہے — یعنی یہ صلاحیت کہ وہ:

1.  دنیا کو ویسے ہی **سمجھیں** جیسے انسان سمجھتے ہیں
2.  **سمجھیں** کہ انسان کیا چاہتے ہیں
3.  اہداف کو کیسے حاصل کیا جائے اس بارے میں **استدلال کریں**
4.  چیزوں کو حقیقت بنانے کے لیے **عمل کریں**

اب یہ صرف انجینئرنگ نہیں ہے۔ یہ دماغ بنانا ہے۔

---

## خلاصہ

-   **VLA** ایک ہی ماڈل میں وژن، لینگویج، اور ایکشن کو یکجا کرتا ہے
-   روبوٹس کو قدرتی زبان کے کمانڈز کو سمجھنے کے قابل بناتا ہے
-   اہم آرکیٹیکچرز: RT-2، OpenVLA، GR00T
-   یہ روبوٹکس AI کا جدید ترین کنارہ ہے

---

## آئیے ایک دماغ بنائیں

اگلا: ہم آواز سے شروع کریں گے — اپنے روبوٹ کو تقریر سمجھنا سکھائیں گے۔

**→ [وائس ٹو ایکشن: Whisper انٹیگریشن](/docs/module-4-vla/02-voice-to-action)**