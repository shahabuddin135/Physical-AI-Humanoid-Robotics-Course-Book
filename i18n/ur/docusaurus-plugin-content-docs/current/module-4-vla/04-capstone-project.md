---
description: 'Ø¹Ø¸ÛŒÙ… Ø§Ù„Ø´Ø§Ù† Ø§Ø®ØªØªØ§Ù…Û” Ø§ÛŒÚ© Ø®ÙˆØ¯ Ù…Ø®ØªØ§Ø± ÛÛŒÙˆÙ…Ù†Ø§Ø¦ÛŒÚˆ Ø¨Ù†Ø§Ø¦ÛŒÚºÛ” Ø¬Ùˆ ÛØ± Ú†ÛŒØ² Ú©Ùˆ ÛŒÚ©Ø¬Ø§
  Ú©Ø±ØªØ§ ÛÛ’: ROS 2, simulation, GPU perception, navigation, voice commands, Ø§ÙˆØ± LLM-based
  planning.'
id: vla-capstone
keywords:
- capstone
- autonomous robot
- VLA
- integration
- final project
module: 4
sidebar_position: 4
tags:
- module-4
- capstone
- advanced
title: 'Ú©Ø§Ù¾ Ø§Ø³Ù¹ÙˆÙ†: Ø¢Ù¹ÙˆÙ†ÙˆÙ…Ø³ ÛÛŒÙˆÙ…Ù†Ø§Ø¦ÛŒÚˆ'
---

# Ú©ÛŒÙ¾ Ø§Ø³Ù¹ÙˆÙ†: Ø®ÙˆØ¯ Ù…Ø®ØªØ§Ø± ÛÛŒÙˆÙ…Ù†Ø§Ø¦ÛŒÚˆ

> **TL;DR:** Ú†Ø§Ø± Ù…Ø§ÚˆÛŒÙˆÙ„Ø²Û” Ø¯Ø±Ø¬Ù†ÙˆÚº ØªØµÙˆØ±Ø§ØªÛ” Ø§ÛŒÚ© Ø±ÙˆØ¨ÙˆÙ¹Û” ÛŒÛ Ú©ÛŒÙ¾ Ø§Ø³Ù¹ÙˆÙ† Ø¢Ù¾ Ú©ÛŒ Ø³ÛŒÚ©Ú¾ÛŒ ÛÙˆØ¦ÛŒ ÛØ± Ú†ÛŒØ² Ú©Ùˆ Ø§ÛŒÚ© Ø®ÙˆØ¯ Ù…Ø®ØªØ§Ø± ÛÛŒÙˆÙ…Ù†Ø§Ø¦ÛŒÚˆ Ù…ÛŒÚº Ø¶Ù… Ú©Ø±ØªØ§ ÛÛ’ Ø¬Ùˆ ØµÙˆØªÛŒ Ú©Ù…Ø§Ù†ÚˆØ² Ú©Ùˆ Ø³Ù…Ø¬Ú¾ Ø³Ú©ØªØ§ ÛÛ’ØŒ Ø§Ù¾Ù†Û’ Ù…Ø§Ø­ÙˆÙ„ Ú©Ùˆ Ù…Ø­Ø³ÙˆØ³ Ú©Ø± Ø³Ú©ØªØ§ ÛÛ’ØŒ Ú©Ø§Ù…ÙˆÚº Ú©Û’ Ø¨Ø§Ø±Û’ Ù…ÛŒÚº Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ú©Ø± Ø³Ú©ØªØ§ ÛÛ’ØŒ Ø§ÙˆØ± Ù¾ÛŒÚ†ÛŒØ¯Û Ø§Ø¹Ù…Ø§Ù„ Ø§Ù†Ø¬Ø§Ù… Ø¯Û’ Ø³Ú©ØªØ§ ÛÛ’Û” ÛŒÛ Ø¢Ø®Ø±ÛŒ Ø¨Ø§Ø³ ÛÛ’Û”

---

## Ø¢Ù¾ Ú©ÛŒØ§ Ø¨Ù†Ø§ Ø±ÛÛ’ ÛÛŒÚº

Ø§ÛŒÚ© Ø®ÙˆØ¯ Ù…Ø®ØªØ§Ø± ÛÛŒÙˆÙ…Ù†Ø§Ø¦ÛŒÚˆ Ø±ÙˆØ¨ÙˆÙ¹ Ø¬Ùˆ ÛŒÛ Ú©Ø± Ø³Ú©ØªØ§ ÛÛ’:

| ØµÙ„Ø§Ø­ÛŒØª | Ù…Ø§Ø®Ø° Ù…Ø§ÚˆÛŒÙˆÙ„ |
|------------|---------------|
| âœ… ROS 2 communication | Module 1 |
| âœ… Physics simulation | Module 2 |
| âœ… GPU-accelerated perception | Module 3 |
| âœ… Autonomous navigation | Module 3 |
| âœ… Voice command understanding | Module 4 |
| âœ… LLM-based task planning | Module 4 |
| âœ… Object manipulation | NEW |

---

## Ù…Ú©Ù…Ù„ Ø¢Ø±Ú©ÛŒÙ¹ÛŒÚ©Ú†Ø±

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           THE AUTONOMOUS HUMANOID                            â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚                           COGNITIVE LAYER                                â”‚â”‚
â”‚  â”‚                                                                          â”‚â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚â”‚
â”‚  â”‚   â”‚   Whisper   â”‚â”€â”€â”€â–¶â”‚ LLM Planner â”‚â”€â”€â”€â–¶â”‚Task Executorâ”‚                â”‚â”‚
â”‚  â”‚   â”‚  (Speech)   â”‚    â”‚  (GPT-4)    â”‚    â”‚  (Sequencer)â”‚                â”‚â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                â”‚â”‚
â”‚  â”‚                                                â”‚                         â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                   â”‚                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚                        PERCEPTION LAYER        â”‚                         â”‚â”‚
â”‚  â”‚                                                â”‚                         â”‚â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”                   â”‚â”‚
â”‚  â”‚   â”‚   cuVSLAM   â”‚    â”‚   NVBlox    â”‚    â”‚ Object    â”‚                   â”‚â”‚
â”‚  â”‚   â”‚  (Mapping)  â”‚    â”‚  (3D Map)   â”‚    â”‚ Detection â”‚                   â”‚â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                   â”‚â”‚
â”‚  â”‚          â”‚                  â”‚                 â”‚                          â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚             â”‚                  â”‚                 â”‚                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚          â”‚     NAVIGATION   â”‚     LAYER       â”‚                          â”‚â”‚
â”‚  â”‚          â”‚                  â”‚                 â”‚                          â”‚â”‚
â”‚  â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                 â”‚                          â”‚â”‚
â”‚  â”‚                      â–¼                        â”‚                          â”‚â”‚
â”‚  â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”                 â”‚â”‚
â”‚  â”‚              â”‚    Nav2     â”‚          â”‚  Manipulation â”‚                 â”‚â”‚
â”‚  â”‚              â”‚  (Planning) â”‚          â”‚   Controller  â”‚                 â”‚â”‚
â”‚  â”‚              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚â”‚
â”‚  â”‚                     â”‚                         â”‚                          â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                        â”‚                         â”‚                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚                     â”‚    MOTOR LAYER          â”‚                          â”‚â”‚
â”‚  â”‚                     â–¼                         â–¼                          â”‚â”‚
â”‚  â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚â”‚
â”‚  â”‚              â”‚ Locomotion  â”‚          â”‚    Arm      â”‚                   â”‚â”‚
â”‚  â”‚              â”‚  Controller â”‚          â”‚ Controller  â”‚                   â”‚â”‚
â”‚  â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚â”‚
â”‚  â”‚                                                                          â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                              â”‚
â”‚                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚                          â”‚   ISAAC SIM     â”‚                                â”‚
â”‚                          â”‚   (Physics)     â”‚                                â”‚
â”‚                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Ù¾Ø±ÙˆØ¬ÛŒÚ©Ù¹ Ú©Ø§ ÚˆÚ¾Ø§Ù†Ú†Û

```
autonomous_humanoid/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ nav2_params.yaml
â”‚   â”œâ”€â”€ robot_params.yaml
â”‚   â””â”€â”€ perception_params.yaml
â”œâ”€â”€ launch/
â”‚   â”œâ”€â”€ simulation.launch.py
â”‚   â”œâ”€â”€ perception.launch.py
â”‚   â”œâ”€â”€ navigation.launch.py
â”‚   â””â”€â”€ full_system.launch.py
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ speech_recognition_node.py
â”‚   â”œâ”€â”€ llm_planner_node.py
â”‚   â”œâ”€â”€ task_executor_node.py
â”‚   â”œâ”€â”€ world_state_node.py
â”‚   â”œâ”€â”€ manipulation_node.py
â”‚   â””â”€â”€ locomotion_bridge.py
â”œâ”€â”€ models/
â”‚   â””â”€â”€ (Whisper, object detection weights)
â”œâ”€â”€ CMakeLists.txt
â””â”€â”€ package.xml
```

---

## Ù…Ø±Ø­Ù„Û 1: ÙˆØ±Ù„Úˆ Ø§Ø³Ù¹ÛŒÙ¹ Ù…ÛŒÙ†ÛŒØ¬Ø±

Ø±ÙˆØ¨ÙˆÙ¹ Ú©Û’ Ø¹Ù„Ù… Ú©Ø§ Ù…Ø±Ú©Ø²ÛŒ Ù…Ø±Ú©Ø²:

```python
#!/usr/bin/env python3
"""
world_state_node.py
Aggregates all perception into unified world state.
"""

import rclpy
from rclpy.node import Node
from std_msgs.msg import String
from geometry_msgs.msg import PoseStamped, PoseWithCovarianceStamped
from sensor_msgs.msg import Image
from vision_msgs.msg import Detection3DArray
import json
import numpy as np

class WorldStateNode(Node):
    def __init__(self):
        super().__init__('world_state')
        
        # State storage
        self.state = {
            'robot': {
                'location': None,
                'holding': None,
                'battery': 100,
            },
            'objects': {},
            'locations': {
                'kitchen': {'x': 5.0, 'y': 3.0},
                'living_room': {'x': 2.0, 'y': 1.0},
                'charging_station': {'x': 0.0, 'y': 0.0},
            },
            'people': [],
        }
        
        # Subscribers
        self.pose_sub = self.create_subscription(
            PoseWithCovarianceStamped, '/amcl_pose', 
            self.pose_callback, 10
        )
        
        self.detections_sub = self.create_subscription(
            Detection3DArray, '/detections',
            self.detections_callback, 10
        )
        
        # Publisher
        self.state_pub = self.create_publisher(String, '/world_state', 10)
        
        # Timer
        self.timer = self.create_timer(0.5, self.publish_state)
        
        self.get_logger().info('World State Manager ready!')
    
    def pose_callback(self, msg):
        """Update robot pose."""
        self.state['robot']['location'] = {
            'x': msg.pose.pose.position.x,
            'y': msg.pose.pose.position.y,
            'theta': self.quaternion_to_yaw(msg.pose.pose.orientation)
        }
        
        # Determine named location
        for name, pos in self.state['locations'].items():
            dist = np.sqrt(
                (pos['x'] - msg.pose.pose.position.x)**2 +
                (pos['y'] - msg.pose.pose.position.y)**2
            )
            if dist < 1.0:
                self.state['robot']['current_room'] = name
                break
    
    def detections_callback(self, msg):
        """Update detected objects."""
        for detection in msg.detections:
            obj_id = detection.results[0].hypothesis.class_id
            obj_name = self.id_to_name(obj_id)
            
            self.state['objects'][obj_name] = {
                'position': {
                    'x': detection.bbox.center.position.x,
                    'y': detection.bbox.center.position.y,
                    'z': detection.bbox.center.position.z,
                },
                'confidence': detection.results[0].hypothesis.score,
                'last_seen': self.get_clock().now().to_msg().sec
            }
    
    def id_to_name(self, class_id):
        """Map detection ID to object name."""
        names = {
            0: 'cup', 1: 'bottle', 2: 'book', 3: 'phone',
            4: 'remote', 5: 'apple', 6: 'banana'
        }
        return names.get(class_id, f'object_{class_id}')
    
    def quaternion_to_yaw(self, q):
        """Convert quaternion to yaw angle."""
        siny_cosp = 2 * (q.w * q.z + q.x * q.y)
        cosy_cosp = 1 - 2 * (q.y * q.y + q.z * q.z)
        return np.arctan2(siny_cosp, cosy_cosp)
    
    def publish_state(self):
        """Publish current world state."""
        msg = String()
        msg.data = json.dumps(self.state)
        self.state_pub.publish(msg)
    
    def get_summary(self):
        """Get human-readable summary."""
        robot = self.state['robot']
        room = robot.get('current_room', 'unknown')
        holding = robot.get('holding', 'nothing')
        objects = list(self.state['objects'].keys())
        
        return f"""Robot is in {room}, holding {holding}.
Visible objects: {', '.join(objects) if objects else 'none'}"""

def main(args=None):
    rclpy.init(args=args)
    node = WorldStateNode()
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
```

---

## Ù…Ø±Ø­Ù„Û 2: Ù¹Ø§Ø³Ú© Ø§ÛŒÚ¯Ø²ÛŒÚ©ÛŒÙˆÙ¹Ø±

Ù¾Ù„Ø§Ù†Ø± Ø³Û’ Ø§ÛŒÚ©Ø´Ù† Ø³ÛŒÚ©ÙˆÛŒÙ†Ø³Ø² Ú†Ù„Ø§ØªØ§ ÛÛ’:

```python
#!/usr/bin/env python3
"""
task_executor_node.py
Executes task plans from the LLM planner.
"""

import rclpy
from rclpy.node import Node
from rclpy.action import ActionClient
from std_msgs.msg import String
from geometry_msgs.msg import PoseStamped
from nav2_msgs.action import NavigateToPose
import json
import time

class TaskExecutorNode(Node):
    def __init__(self):
        super().__init__('task_executor')
        
        # Action clients
        self.nav_client = ActionClient(self, NavigateToPose, 'navigate_to_pose')
        
        # Subscribers
        self.plan_sub = self.create_subscription(
            String, '/task_plan', self.plan_callback, 10
        )
        
        self.world_state_sub = self.create_subscription(
            String, '/world_state', self.world_state_callback, 10
        )
        
        # Publishers
        self.status_pub = self.create_publisher(String, '/task_status', 10)
        self.speak_pub = self.create_publisher(String, '/speak', 10)
        self.grasp_pub = self.create_publisher(String, '/grasp_command', 10)
        
        # State
        self.world_state = {}
        self.current_task = None
        self.is_executing = False
        
        # Location map
        self.locations = {
            'kitchen': (5.0, 3.0, 0.0),
            'living room': (2.0, 1.0, 0.0),
            'bedroom': (8.0, 2.0, 0.0),
            'bathroom': (6.0, 6.0, 1.57),
        }
        
        self.get_logger().info('Task Executor ready!')
    
    def world_state_callback(self, msg):
        """Update world state."""
        self.world_state = json.loads(msg.data)
    
    def plan_callback(self, msg):
        """Execute incoming task plan."""
        if self.is_executing:
            self.get_logger().warn('Already executing a task!')
            return
        
        plan = json.loads(msg.data)
        self.execute_plan(plan)
    
    def execute_plan(self, plan):
        """Execute all steps in the plan."""
        self.is_executing = True
        self.current_task = plan.get('task', 'unknown')
        
        self.publish_status('started', f"Starting: {self.current_task}")
        
        steps = plan.get('steps', [])
        
        for i, step in enumerate(steps):
            self.publish_status('in_progress', 
                f"Step {i+1}/{len(steps)}: {step['action']}")
            
            success = self.execute_step(step)
            
            if not success:
                self.publish_status('failed', 
                    f"Failed at step {i+1}: {step['action']}")
                break
        else:
            self.publish_status('completed', 
                f"Completed: {self.current_task}")
        
        self.is_executing = False
    
    def execute_step(self, step):
        """Execute a single action step."""
        action = step['action']
        target = step.get('target', '')
        params = step.get('params', {})
        
        handlers = {
            'navigate': self.handle_navigate,
            'look': self.handle_look,
            'grasp': self.handle_grasp,
            'place': self.handle_place,
            'speak': self.handle_speak,
            'wait': self.handle_wait,
        }
        
        handler = handlers.get(action)
        if handler:
            return handler(target, params)
        else:
            self.get_logger().warn(f'Unknown action: {action}')
            return False
    
    def handle_navigate(self, target, params):
        """Navigate to target location."""
        # Check if it's a named location
        if target.lower() in self.locations:
            x, y, theta = self.locations[target.lower()]
        elif target in self.world_state.get('objects', {}):
            obj = self.world_state['objects'][target]
            x = obj['position']['x']
            y = obj['position']['y']
            theta = 0.0
        else:
            self.get_logger().error(f'Unknown location: {target}')
            return False
        
        # Create goal
        goal = NavigateToPose.Goal()
        goal.pose.header.frame_id = 'map'
        goal.pose.header.stamp = self.get_clock().now().to_msg()
        goal.pose.pose.position.x = x
        goal.pose.pose.position.y = y
        goal.pose.pose.orientation.w = 1.0
        
        # Send goal
        self.nav_client.wait_for_server()
        future = self.nav_client.send_goal_async(goal)
        rclpy.spin_until_future_complete(self, future)
        
        goal_handle = future.result()
        if not goal_handle.accepted:
            return False
        
        # Wait for result
        result_future = goal_handle.get_result_async()
        rclpy.spin_until_future_complete(self, result_future)
        
        return True
    
    def handle_look(self, target, params):
        """Look at target (head movement)."""
        # Publish look command
        msg = String()
        msg.data = json.dumps({'target': target})
        # self.look_pub.publish(msg)
        time.sleep(1.0)  # Simulate head movement
        return True
    
    def handle_grasp(self, target, params):
        """Grasp an object."""
        msg = String()
        msg.data = json.dumps({
            'action': 'grasp',
            'object': target
        })
        self.grasp_pub.publish(msg)
        time.sleep(3.0)  # Simulate grasp time
        
        # Update state
        self.world_state['robot']['holding'] = target
        return True
    
    def handle_place(self, target, params):
        """Place held object."""
        msg = String()
        msg.data = json.dumps({
            'action': 'place',
            'location': target
        })
        self.grasp_pub.publish(msg)
        time.sleep(2.0)
        
        self.world_state['robot']['holding'] = None
        return True
    
    def handle_speak(self, message, params):
        """Speak to user."""
        msg = String()
        msg.data = message
        self.speak_pub.publish(msg)
        return True
    
    def handle_wait(self, duration, params):
        """Wait for specified time."""
        seconds = float(duration) if duration else params.get('seconds', 1)
        time.sleep(seconds)
        return True
    
    def publish_status(self, status, message):
        """Publish task status."""
        msg = String()
        msg.data = json.dumps({
            'task': self.current_task,
            'status': status,
            'message': message
        })
        self.status_pub.publish(msg)
        self.get_logger().info(f'[{status}] {message}')

def main(args=None):
    rclpy.init(args=args)
    node = TaskExecutorNode()
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
```

---

## Ù…Ø±Ø­Ù„Û 3: Ù…ÛŒÙ† Ù„Ø§Ù†Ú† ÙØ§Ø¦Ù„

```python
# launch/full_system.launch.py
from launch import LaunchDescription
from launch_ros.actions import Node
from launch.actions import IncludeLaunchDescription
from launch.launch_description_sources import PythonLaunchDescriptionSource
from ament_index_python.packages import get_package_share_directory
import os

def generate_launch_description():
    pkg_dir = get_package_share_directory('autonomous_humanoid')
    
    return LaunchDescription([
        # === PERCEPTION ===
        # World State Manager
        Node(
            package='autonomous_humanoid',
            executable='world_state_node',
            name='world_state',
            output='screen',
        ),
        
        # === COGNITIVE ===
        # Speech Recognition
        Node(
            package='autonomous_humanoid',
            executable='speech_recognition_node',
            name='speech_recognition',
            output='screen',
            parameters=[{
                'model_name': 'small',
                'continuous_mode': True,
                'wake_word': 'hey robot',
            }],
        ),
        
        # LLM Planner
        Node(
            package='autonomous_humanoid',
            executable='llm_planner_node',
            name='llm_planner',
            output='screen',
            parameters=[{
                'model': 'gpt-4',
                'temperature': 0.0,
            }],
        ),
        
        # Task Executor
        Node(
            package='autonomous_humanoid',
            executable='task_executor_node',
            name='task_executor',
            output='screen',
        ),
        
        # === NAVIGATION ===
        IncludeLaunchDescription(
            PythonLaunchDescriptionSource(
                os.path.join(pkg_dir, 'launch', 'navigation.launch.py')
            )
        ),
        
        # Locomotion Bridge
        Node(
            package='autonomous_humanoid',
            executable='locomotion_bridge',
            name='locomotion_bridge',
            output='screen',
        ),
    ])
```

---

## Ù…Ø±Ø­Ù„Û 4: ÚˆÛŒÙ…Ùˆ Ø§Ø³Ú©Ø±Ù¾Ù¹

```python
#!/usr/bin/env python3
"""
demo.py
Interactive demo of the autonomous humanoid.
"""

import rclpy
from rclpy.node import Node
from std_msgs.msg import String
import json

class DemoNode(Node):
    def __init__(self):
        super().__init__('demo')
        
        # Publishers
        self.command_pub = self.create_publisher(
            String, '/voice_command', 10
        )
        
        # Subscribers
        self.status_sub = self.create_subscription(
            String, '/task_status', self.status_callback, 10
        )
        
    def status_callback(self, msg):
        """Print status updates."""
        status = json.loads(msg.data)
        print(f"  [{status['status']}] {status['message']}")
    
    def send_command(self, command):
        """Send a voice command."""
        print(f"\nğŸ¤ Command: \"{command}\"")
        print("-" * 40)
        
        msg = String()
        msg.data = command
        self.command_pub.publish(msg)

def main():
    rclpy.init()
    node = DemoNode()
    
    print("=" * 50)
    print("  AUTONOMOUS HUMANOID DEMO")
    print("=" * 50)
    print("\nExample commands:")
    print("  - Bring me the water bottle")
    print("  - Go to the kitchen")
    print("  - Find my phone")
    print("  - What can you see?")
    print("\nType 'quit' to exit.\n")
    
    import threading
    spin_thread = threading.Thread(target=rclpy.spin, args=(node,))
    spin_thread.start()
    
    try:
        while True:
            command = input("\nğŸ¤– Enter command: ").strip()
            
            if command.lower() == 'quit':
                break
            
            if command:
                node.send_command(command)
            
    except KeyboardInterrupt:
        pass
    
    rclpy.shutdown()
    spin_thread.join()

if __name__ == '__main__':
    main()
```

---

## Ù…Ú©Ù…Ù„ Ø³Ø³Ù¹Ù… Ú†Ù„Ø§Ù†Ø§

### Ù¹Ø±Ù…ÛŒÙ†Ù„ 1: Isaac Sim

```bash
./isaac-sim.sh
# Ø§Ù¾Ù†Û’ ÛÛŒÙˆÙ…Ù†Ø§Ø¦ÛŒÚˆ Ø³ÛŒÙ† Ú©Ùˆ Ù„ÙˆÚˆ Ú©Ø±ÛŒÚº Ø§ÙˆØ± Play Ø¯Ø¨Ø§Ø¦ÛŒÚºÛ”
```

### Ù¹Ø±Ù…ÛŒÙ†Ù„ 2: Ù…Ú©Ù…Ù„ Ø³Ø³Ù¹Ù… Ù„Ø§Ù†Ú†

```bash
ros2 launch autonomous_humanoid full_system.launch.py
```

### Ù¹Ø±Ù…ÛŒÙ†Ù„ 3: ÚˆÛŒÙ…Ùˆ

```bash
ros2 run autonomous_humanoid demo
```

---

## Ù…Ø«Ø§Ù„ Ú©Û’ Ø·ÙˆØ± Ù¾Ø± ØªØ¹Ø§Ù…Ù„

```
ğŸ¤– Enter command: Hey robot, bring me the red cup

ğŸ¤ Command: "bring me the red cup"
----------------------------------------
  [started] Starting: bring me the red cup
  [in_progress] Step 1/5: look
  [in_progress] Step 2/5: navigate
  [in_progress] Step 3/5: grasp
  [in_progress] Step 4/5: navigate
  [in_progress] Step 5/5: speak
  [completed] Completed: bring me the red cup

ğŸ¤– Enter command: _
```

---

## ØªÙˆØ³ÛŒØ¹ Ú©Û’ Ù„ÛŒÛ’ Ú†ÛŒÙ„Ù†Ø¬Ø²

| Ú†ÛŒÙ„Ù†Ø¬ | Ù…Ø´Ú©Ù„ | Ù…ÛØ§Ø±ØªÛŒÚº |
|-----------|------------|--------|
| Multi-object manipulation | â­â­ | Arm control |
| Dynamic obstacle avoidance | â­â­ | Real-time planning |
| Multi-room mapping | â­â­â­ | SLAM |
| Human following | â­â­â­ | People tracking |
| Collaborative tasks | â­â­â­â­ | Multi-agent |
| Continuous learning | â­â­â­â­â­ | Online learning |

---

## Ø¢Ù¾ Ù†Û’ Ú©Ø± Ø¯Ú©Ú¾Ø§ÛŒØ§! ğŸ‰

Ø¢Ù¾ Ù†Û’ Ø§ÛŒÚ© Ø®ÙˆØ¯ Ù…Ø®ØªØ§Ø± ÛÛŒÙˆÙ…Ù†Ø§Ø¦ÛŒÚˆ Ø±ÙˆØ¨ÙˆÙ¹ Ø¨Ù†Ø§ÛŒØ§ ÛÛ’ Ø¬Ùˆ:

- âœ… Ù‚Ø¯Ø±ØªÛŒ Ø²Ø¨Ø§Ù† Ú©Û’ Ú©Ù…Ø§Ù†ÚˆØ² Ú©Ùˆ Ø³Ù…Ø¬Ú¾ØªØ§ ÛÛ’
- âœ… GPU Ø§ÛŒÚ©Ø³Ù„Ø±ÛŒØ´Ù† Ú©Û’ Ø³Ø§ØªÚ¾ Ø§Ù¾Ù†Û’ Ù…Ø§Ø­ÙˆÙ„ Ú©Ùˆ Ù…Ø­Ø³ÙˆØ³ Ú©Ø±ØªØ§ ÛÛ’
- âœ… LLMs Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Ú©Ø§Ù…ÙˆÚº Ú©Û’ Ø¨Ø§Ø±Û’ Ù…ÛŒÚº Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ú©Ø±ØªØ§ ÛÛ’
- âœ… Ù¾ÛŒÚ†ÛŒØ¯Û Ø±ÙˆÛŒÙˆÚº Ú©ÛŒ Ù…Ù†ØµÙˆØ¨Û Ø¨Ù†Ø¯ÛŒ Ø§ÙˆØ± Ø§Ù† Ù¾Ø± Ø¹Ù…Ù„ Ø¯Ø±Ø¢Ù…Ø¯ Ú©Ø±ØªØ§ ÛÛ’
- âœ… Ø®ÙˆØ¯ Ù…Ø®ØªØ§Ø± Ø·Ø±ÛŒÙ‚Û’ Ø³Û’ Ù†ÛŒÙˆÛŒÚ¯ÛŒÙ¹ Ú©Ø±ØªØ§ ÛÛ’
- âœ… Ø§Ø´ÛŒØ§Ø¡ Ú©Û’ Ø³Ø§ØªÚ¾ ØªØ¹Ø§Ù…Ù„ Ú©Ø±ØªØ§ ÛÛ’

**ÛŒÛ ÙØ²ÛŒÚ©Ù„ AI Ù…ÛŒÚº Ø¢Ø±Ù¹ Ú©ÛŒ Ø­Ø§Ù„Øª ÛÛ’Û”**

---

## Ø§Ú¯Ù„Ø§ Ú©ÛŒØ§ ÛÛ’ØŸ

Ù…Ø³ØªÙ‚Ø¨Ù„ Ø§Ø¨Ú¾ÛŒ Ù„Ú©Ú¾Ø§ Ø¬Ø§ Ø±ÛØ§ ÛÛ’:

**â†’ [Ø§Ú¯Ù„Ø§ Ú©ÛŒØ§ ÛÛ’: ÙØ²ÛŒÚ©Ù„ AI Ú©Ø§ Ù…Ø³ØªÙ‚Ø¨Ù„](/docs/module-4-vla/05-whats-next)**